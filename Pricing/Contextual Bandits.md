**Main information :**

- Contextual Bandits are an extension of the Multi-armed Bandit problem where the decision-making agent not only receives a reward for each action (or “arm”) but also has access to context or environment-related information before choosing an arm. The context can be any piece of information that might influence the outcome, such as customer demographics or external market conditions.


**Tags**
#contextual_bandits