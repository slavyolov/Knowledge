- https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/
- https://jalammar.github.io/feedforward-neural-networks-visual-interactive/

# Takeaways :

-  “training” a neural network, it just means finding the weights we use to calculate the prediction.
- #gradient_descent : 
	- Calculus - knowing the function we’re minimizing (our loss function, e.g. the average of (y_ hat - y)² for all our data points), and knowing the current inputs into it (the current weight and bias), the derivatives of the loss function tell us which direction to nudge W and b in order to minimize the error.
- Classification problems - using #Softmax to convert continueus valueus into probabilities of being class A, B or C
- 